{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% 实现VGGNet\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "VGG Net\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('VGG Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义一个VGG的block\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "           nn.ReLU(True)]\n",
    "    \n",
    "    for i in range(num_convs-1):\n",
    "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "        \n",
    "    net.append(nn.MaxPool2d(2, 2))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace)\n  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace)\n  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): ReLU(inplace)\n  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "block_demo = vgg_block(3, 64, 128)\n",
    "print(block_demo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([1, 128, 150, 150])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "input_demo = Variable(torch.zeros(1, 64, 300, 300))\n",
    "output_demo = block_demo(input_demo)\n",
    "print(output_demo.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# 定义一个函数对block做堆叠\n",
    "def vgg_stack(num_convs, channels):\n",
    "    net = []\n",
    "    for n, c in zip(num_convs, channels):\n",
    "        in_c = c[0]\n",
    "        out_c = c[1]\n",
    "        net.append(vgg_block(n, in_c, out_c))\n",
    "        \n",
    "    return nn.Sequential(*net)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (1): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (2): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (3): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (4): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "vgg_net = vgg_stack((1,1,2,2,2), ((3,64), (64, 128), (128,256), (256, 512),(512,512)))\n",
    "print(vgg_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "test_x = Variable(torch.zeros(1, 3, 256, 256))\n",
    "test_y = vgg_net(test_x)\n",
    "print(test_y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = vgg_net\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 用vgg_net构造vgg网络\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "#  定义一个训练函数\n",
    "import time\n",
    "\n",
    "def train(net, train_data, test_data, epoch, optimizer, criterion):\n",
    "    # \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    for i in range(epoch):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        # 训练阶段 \n",
    "        net.train()\n",
    "        for im, label in train_data:\n",
    "            im = Variable(im)\n",
    "            label = Variable(label)\n",
    "            im = im.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            out = net(im)\n",
    "            loss = criterion(out, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = out.max(1)\n",
    "            num_correct = (pred==label).sum().item()\n",
    "            acc = num_correct/im.shape[0]\n",
    "            train_acc += acc\n",
    "        \n",
    "        # 测试阶段\n",
    "        net.eval()\n",
    "        for im, label in test_data:\n",
    "            im = Variable(im)\n",
    "            label = Variable(label)\n",
    "            im = im.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            out = net(im)\n",
    "            loss = criterion(out, label)\n",
    "            valid_loss += loss.item()\n",
    "            _, pred = out.max(1)\n",
    "            num_correct = (pred==label).sum().item()\n",
    "            acc = num_correct/im.shape[0]\n",
    "            valid_acc += acc\n",
    "        \n",
    "        end = time.time()\n",
    "        time_use = end - start\n",
    "        # 输出本轮结果\n",
    "        print('Epoch {}, Train Loss: {:.6f}, Train Acc: {:.6f},\\\n",
    "         Valid Loss: {:.6f}, Valid Acc: {:.6f}, Time: {:.6f}, len(train_data):{}'\n",
    "              .format(i, \n",
    "                      train_loss/len(train_data), \n",
    "                      train_acc/len(train_data), \n",
    "                      valid_loss/len(test_data), \n",
    "                      valid_acc/len(test_data), \n",
    "                      time_use,\n",
    "                      len(train_data)))\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32')/255\n",
    "    x = (x-0.5)/0.5\n",
    "    x = x.transpose((2, 0, 1)) # channel 放在第一维。\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "train_set = CIFAR10('../data', train=True, transform=data_tf)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_set = CIFAR10('../data', train=False, transform=data_tf)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 0, Train Loss: 2.303468, Train Acc: 0.101368,         Valid Acc: 2.303345, Valid Acc: 0.099840, Time: 45.960831, len(train_data):1563\n",
      "Epoch 1, Train Loss: 2.269176, Train Acc: 0.120022,         Valid Acc: 2.104997, Valid Acc: 0.236222, Time: 45.342551, len(train_data):1563\n",
      "Epoch 2, Train Loss: 1.814172, Train Acc: 0.296985,         Valid Acc: 1.615363, Valid Acc: 0.382488, Time: 45.372320, len(train_data):1563\n",
      "Epoch 3, Train Loss: 1.458140, Train Acc: 0.451516,         Valid Acc: 1.292550, Valid Acc: 0.530351, Time: 45.358127, len(train_data):1563\n",
      "Epoch 4, Train Loss: 1.139639, Train Acc: 0.586852,         Valid Acc: 1.088528, Valid Acc: 0.605132, Time: 45.428266, len(train_data):1563\n",
      "Epoch 5, Train Loss: 0.895896, Train Acc: 0.685141,         Valid Acc: 1.024470, Valid Acc: 0.642372, Time: 46.262565, len(train_data):1563\n",
      "Epoch 6, Train Loss: 0.718106, Train Acc: 0.752499,         Valid Acc: 0.816562, Valid Acc: 0.719748, Time: 45.991889, len(train_data):1563\n",
      "Epoch 7, Train Loss: 0.574288, Train Acc: 0.802443,         Valid Acc: 0.777120, Valid Acc: 0.754493, Time: 45.641105, len(train_data):1563\n",
      "Epoch 8, Train Loss: 0.455952, Train Acc: 0.844050,         Valid Acc: 0.754586, Valid Acc: 0.762480, Time: 46.010470, len(train_data):1563\n",
      "Epoch 9, Train Loss: 0.357999, Train Acc: 0.878519,         Valid Acc: 0.734181, Valid Acc: 0.779653, Time: 45.722448, len(train_data):1563\n",
      "Epoch 10, Train Loss: 0.283986, Train Acc: 0.901691,         Valid Acc: 0.793235, Valid Acc: 0.766973, Time: 46.710128, len(train_data):1563\n",
      "Epoch 11, Train Loss: 0.224526, Train Acc: 0.922645,         Valid Acc: 0.749480, Valid Acc: 0.784245, Time: 46.108272, len(train_data):1563\n",
      "Epoch 12, Train Loss: 0.177993, Train Acc: 0.939979,         Valid Acc: 0.780993, Valid Acc: 0.791434, Time: 45.475732, len(train_data):1563\n",
      "Epoch 13, Train Loss: 0.139539, Train Acc: 0.951715,         Valid Acc: 0.891607, Valid Acc: 0.777456, Time: 45.128775, len(train_data):1563\n",
      "Epoch 14, Train Loss: 0.114782, Train Acc: 0.961672,         Valid Acc: 0.889634, Valid Acc: 0.781949, Time: 45.162713, len(train_data):1563\n",
      "Epoch 15, Train Loss: 0.105565, Train Acc: 0.964291,         Valid Acc: 0.919689, Valid Acc: 0.791534, Time: 45.165130, len(train_data):1563\n",
      "Epoch 16, Train Loss: 0.083110, Train Acc: 0.973389,         Valid Acc: 1.016699, Valid Acc: 0.789537, Time: 45.171941, len(train_data):1563\n",
      "Epoch 17, Train Loss: 0.074221, Train Acc: 0.975368,         Valid Acc: 1.029730, Valid Acc: 0.785144, Time: 45.133668, len(train_data):1563\n",
      "Epoch 18, Train Loss: 0.059270, Train Acc: 0.980906,         Valid Acc: 1.058595, Valid Acc: 0.793431, Time: 45.172413, len(train_data):1563\n",
      "Epoch 19, Train Loss: 0.057514, Train Acc: 0.981406,         Valid Acc: 0.991949, Valid Acc: 0.797724, Time: 45.110547, len(train_data):1563\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 开始训练\n",
    "net = vgg()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(net, train_data, test_data, 20, optimizer, criterion)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Regular dictionary\na A\nb B\nc C\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# orderDict\n",
    "import collections\n",
    "print(\"Regular dictionary\")\n",
    "d = {}\n",
    "d['a'] = 'A'\n",
    "d['b'] = 'B'\n",
    "d['c'] = 'C'\n",
    "for k ,v in d.items():\n",
    "    print(k, v)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Order dictionary\na A\nb B\nc C\n2 2\n1 1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Order dictionary')\n",
    "d1 = collections.OrderedDict()\n",
    "d1['a'] = 'A'\n",
    "d1['b'] = 'B'\n",
    "d1['c'] = 'C'\n",
    "d1['2'] = '2'\n",
    "d1['1'] = '1'\n",
    "for k, v in d1.items():\n",
    "    print(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "one(1,2,3,4,5,6):\n(2, 3, 4, 5, 6)\none(*c):\n(7, 8, 9)\none(*d1):\n('b', 'c', '2', '1')\none(c):\n()\none(d1):\n()\ntwo(c):\n([6, 7, 8, 9],)\ntwo(d1):\n(OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('2', '2'), ('1', '1')]),)\ntwo(*c):\n(6, 7, 8, 9)\ntwo(*d1):\n('a', 'b', 'c', '2', '1')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 单个星号代表这个位置接收任意多个非关键字参数，并转化成元表。\n",
    "# 也就是b 会接受除了a之外的剩下的非关键字参数，需要注意的是加在形参面前代表的是收集参数，\n",
    "# 如果*号加在了是实参上(例如第十四行)，代表的是将输入迭代器拆成一个个元素\n",
    "def one(a, *b):\n",
    "    print(b)\n",
    "\n",
    "def two(*b):\n",
    "    print(b)\n",
    "\n",
    "c = [6, 7, 8, 9]\n",
    "print('one(1,2,3,4,5,6):')\n",
    "one(1,2,3,4,5,6)\n",
    "print('one(*c):')\n",
    "one(*c)\n",
    "print('one(*d1):')\n",
    "one(*d1)\n",
    "print('one(c):')\n",
    "one(c)\n",
    "print('one(d1):')\n",
    "one(d1)\n",
    "print('two(c):')\n",
    "two(c)\n",
    "print('two(d1):')\n",
    "two(d1)\n",
    "print('two(*c):')\n",
    "two(*c)\n",
    "print('two(*d1):')\n",
    "two(*d1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-12c6a786",
   "language": "python",
   "display_name": "PyCharm (PyTorchTutorals)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}