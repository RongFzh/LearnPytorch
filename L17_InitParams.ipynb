{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "17. 参数初始化\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('17. 参数初始化')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Linear(30,40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.1393,  0.0342, -0.1013,  ..., -0.0659, -0.1605, -0.1082],\n        [ 0.0688,  0.1410,  0.0753,  ..., -0.1194, -0.0361,  0.0570],\n        [-0.0370, -0.0980, -0.1168,  ..., -0.0629,  0.1133, -0.1317],\n        ...,\n        [ 0.0582,  0.1643, -0.1362,  ..., -0.0303, -0.0705,  0.0248],\n        [-0.0337,  0.1442, -0.0691,  ...,  0.0006,  0.0549,  0.0117],\n        [ 0.0756, -0.0278, -0.0100,  ...,  0.0618,  0.0627, -0.0785]],\n       requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "w1 = net1[0].weight\n",
    "b1 = net1[0].bias\n",
    "print(w1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[4.7950, 4.6385, 4.2758,  ..., 3.2682, 4.6645, 3.7130],\n        [4.9174, 4.9571, 3.4524,  ..., 4.2832, 4.3980, 4.3547],\n        [3.0903, 3.6660, 4.3986,  ..., 4.5817, 4.7823, 4.9164],\n        ...,\n        [3.7550, 3.2415, 4.5328,  ..., 4.8928, 3.2219, 3.3716],\n        [3.3365, 4.4923, 4.5034,  ..., 3.8579, 4.5991, 3.3664],\n        [4.0578, 4.8959, 4.7040,  ..., 4.0524, 3.8360, 4.9499]],\n       dtype=torch.float64, requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net1[0].weight.data = torch.from_numpy(np.random.uniform(3,5,size=(40,30)))\n",
    "print(net1[0].weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "for layer in net1:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        param_shape = layer.weight.shape\n",
    "        n_in = layer.in_features\n",
    "        n_out = layer.out_features\n",
    "        layer.weight.data = torch.from_numpy(\n",
    "            np.random.normal(-np.sqrt(6.0/(n_in+n_out)), \n",
    "                             np.sqrt(6.0/(n_in+n_out)), \n",
    "                             size=param_shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class sim_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sim_net, self).__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(30,40),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l1[0].weight.data = torch.randn(40,30)\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(40,50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Linear(50,10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=30, out_features=40, bias=True)\n  (1): ReLU()\n)\nSequential(\n  (0): Linear(in_features=40, out_features=50, bias=True)\n  (1): ReLU()\n)\nSequential(\n  (0): Linear(in_features=50, out_features=10, bias=True)\n  (1): ReLU()\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net2 = sim_net()\n",
    "for i in net2.children():\n",
    "    print(i)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sim_net(\n  (l1): Sequential(\n    (0): Linear(in_features=30, out_features=40, bias=True)\n    (1): ReLU()\n  )\n  (l2): Sequential(\n    (0): Linear(in_features=40, out_features=50, bias=True)\n    (1): ReLU()\n  )\n  (l3): Sequential(\n    (0): Linear(in_features=50, out_features=10, bias=True)\n    (1): ReLU()\n  )\n)\nSequential(\n  (0): Linear(in_features=30, out_features=40, bias=True)\n  (1): ReLU()\n)\nLinear(in_features=30, out_features=40, bias=True)\nReLU()\nSequential(\n  (0): Linear(in_features=40, out_features=50, bias=True)\n  (1): ReLU()\n)\nLinear(in_features=40, out_features=50, bias=True)\nReLU()\nSequential(\n  (0): Linear(in_features=50, out_features=10, bias=True)\n  (1): ReLU()\n)\nLinear(in_features=50, out_features=10, bias=True)\nReLU()\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in net2.modules():\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "for layer in net2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        param_shape = layer.weight.shape\n",
    "        layer.weight.data = torch.from_numpy(np.random.normal(0,0.5,size=param_shape))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "30\n40\ntensor([[ 0.0670,  0.5856,  0.9814,  ...,  0.7756, -1.0845, -0.0565],\n        [-0.0133,  0.0352,  0.0382,  ..., -0.6595,  0.5797, -0.8109],\n        [ 0.4446,  0.7263,  0.1032,  ..., -0.1998, -0.3925, -0.3667],\n        ...,\n        [ 0.3232,  0.1433, -0.4798,  ...,  0.5533, -0.3939, -0.4786],\n        [ 0.5342, -0.5778,  0.4263,  ..., -0.1004,  0.2145, -0.3758],\n        [ 0.0881, -0.7196, -0.2196,  ...,  0.4783,  0.1176,  0.9371]],\n       dtype=torch.float64)\n40\n50\ntensor([[ 0.0145, -0.6655, -1.1905,  ...,  0.2446,  0.4366, -0.2178],\n        [ 1.0316,  0.4082,  0.0238,  ...,  0.1800,  0.2315,  0.9798],\n        [ 0.0129, -0.9935, -0.2897,  ...,  0.4993,  0.5429, -0.2808],\n        ...,\n        [-0.1920,  0.3836,  0.0366,  ...,  0.8285,  0.1429,  0.9746],\n        [-0.2358, -0.2187, -0.5549,  ...,  0.3022, -0.0873, -0.5389],\n        [-0.9649,  0.3745,  0.6522,  ...,  0.5665,  0.3041, -0.1081]],\n       dtype=torch.float64)\n50\n10\ntensor([[-1.3472e-01,  3.9765e-01, -4.0193e-01,  5.8640e-01,  3.3785e-01,\n          9.9785e-02, -1.3482e-01, -1.8585e-01,  1.1019e-02,  4.2590e-01,\n         -4.3916e-01,  3.2947e-01, -1.0308e+00, -8.0629e-01, -1.1413e-01,\n         -1.9503e-01,  4.9673e-01, -9.3991e-01,  1.9382e-01, -2.0080e-01,\n         -3.1043e-01, -3.9140e-01, -1.1682e-01, -8.4995e-01, -4.5361e-01,\n          6.8807e-01,  4.2341e-01, -1.6613e-01, -1.6956e-01, -8.3307e-01,\n         -2.8024e-01, -9.1700e-01,  1.1272e-01,  7.3158e-01, -3.5177e-01,\n          2.3998e-01, -8.0457e-01,  1.0992e-01,  4.2335e-01, -1.1749e-01,\n         -3.9593e-01,  4.0802e-01,  2.9670e-01, -3.2077e-01, -6.4354e-01,\n          1.0449e+00, -3.8732e-01, -4.4418e-01, -2.6746e-02,  1.1933e-01],\n        [ 2.8209e-01,  7.9573e-01,  3.8157e-01, -6.0210e-01,  1.3268e-01,\n         -3.6044e-01, -2.7541e-03,  2.4685e-01, -3.6573e-01, -4.6704e-01,\n         -8.4210e-02, -6.5936e-01,  3.4473e-01, -8.0984e-02,  3.3828e-01,\n          6.0019e-01,  6.7028e-02, -6.4550e-01,  2.5828e-02,  1.5955e-02,\n          5.4092e-02,  3.6399e-01, -2.0183e-01,  2.8385e-01, -5.4535e-01,\n          1.0780e-01,  7.5174e-01, -4.3247e-01, -1.9715e-01,  2.0961e-01,\n         -5.9722e-01,  4.6316e-01,  5.2749e-01,  4.2685e-01,  6.3113e-01,\n          7.6405e-01,  6.0953e-02, -2.5516e-01, -1.2368e-01, -6.2796e-01,\n          1.9783e-01, -1.5719e-02,  1.4801e-02, -3.4723e-01, -2.6531e-01,\n          1.0774e-01, -1.3386e-01, -2.5435e-01,  4.7345e-02,  3.1746e-01],\n        [ 3.0025e-01,  4.6428e-01, -5.9462e-01,  6.5900e-02, -4.7909e-01,\n         -9.1436e-01, -5.0089e-01, -6.5102e-01,  9.7016e-01, -4.2579e-01,\n          2.2693e-01, -2.8974e-01,  1.5644e-01, -2.4736e-01,  3.3715e-01,\n          1.7450e-01,  8.0781e-01,  1.6328e-01, -2.5063e-01,  5.9590e-02,\n          3.3791e-01, -7.3473e-01, -3.1212e-01, -4.3529e-01,  2.6597e-01,\n         -7.6968e-02, -2.0151e-01,  2.5316e-02,  8.6232e-01,  5.8263e-01,\n         -4.6123e-02,  9.6679e-01, -7.7158e-01,  5.2930e-01, -2.4320e-01,\n          4.1638e-01,  5.2666e-01, -6.6330e-01,  1.2579e-01, -1.2763e+00,\n          9.2629e-02, -4.6656e-01, -8.5195e-01,  3.9897e-02,  4.0961e-01,\n         -5.9323e-01, -1.6838e-02, -5.5787e-01,  2.3551e-01,  2.5229e-01],\n        [ 8.6262e-02, -6.5688e-02, -7.1168e-01,  1.1048e+00,  6.1263e-01,\n          5.8889e-01,  3.4114e-01, -2.5378e-01, -4.3394e-01,  7.8991e-01,\n         -4.7392e-01,  9.0717e-01,  2.4332e-01, -3.0732e-01, -1.0780e-01,\n         -4.1711e-02, -3.1437e-01, -1.5694e-02,  9.1047e-01,  1.0504e+00,\n         -6.6420e-01,  4.1539e-02, -8.8153e-01, -3.1203e-01,  2.9845e-01,\n         -6.5422e-02,  3.8652e-01, -8.3745e-01,  6.1489e-01, -6.2675e-01,\n          8.4395e-01, -6.1814e-01,  5.1074e-01,  1.4304e-01, -9.0179e-01,\n          1.0411e+00, -3.2982e-01,  2.3115e-01, -4.0690e-01,  1.0912e+00,\n          1.2857e-02, -3.3846e-02,  5.5286e-01, -1.7983e-02,  8.3478e-01,\n         -5.4993e-01,  7.0131e-01,  1.4854e-01,  1.0685e-01,  6.2148e-01],\n        [ 6.2198e-01,  4.5172e-01,  9.0737e-02, -9.2568e-01,  3.2106e-01,\n         -1.8337e-01, -7.0662e-01, -5.5342e-01, -4.4644e-01, -6.5406e-01,\n         -4.8914e-02, -6.3607e-01, -6.4426e-01,  2.9043e-02, -5.6017e-01,\n         -2.9030e-01, -5.9204e-01,  1.4503e-01, -4.8349e-02, -2.8815e-01,\n          3.1256e-01,  3.6293e-02,  2.6398e-01, -3.0543e-01, -6.8946e-01,\n         -1.2639e-01,  1.4199e-01,  4.2882e-01, -1.0128e+00,  3.3132e-01,\n          1.2755e-01,  9.8834e-02,  2.3725e-01,  3.5178e-01, -5.3907e-01,\n         -3.6504e-01,  2.9468e-01, -1.5222e-01,  6.8973e-01, -1.0510e+00,\n          2.9520e-02, -4.1420e-01, -4.0186e-01, -2.8105e-01,  2.0898e-01,\n         -2.1240e-01,  4.3005e-01, -6.2396e-01, -6.2107e-01, -3.0905e-01],\n        [ 4.1977e-01, -7.1920e-02, -9.1540e-02, -2.1055e-02, -6.0759e-01,\n         -8.9552e-01, -4.8635e-01,  3.4743e-01, -1.3220e-01, -4.3964e-01,\n          6.7585e-01, -1.3134e+00,  2.2282e-01,  1.3315e-01,  1.1125e-01,\n         -2.8005e-01, -6.3569e-01, -5.3081e-02,  5.0329e-01,  7.2051e-01,\n         -6.9891e-01, -3.0265e-01,  1.4032e-01, -6.3802e-01, -3.8937e-01,\n         -4.1497e-01, -1.6983e-02, -3.7239e-02, -2.0133e-01,  2.6462e-01,\n         -4.3953e-01, -2.0595e-01,  9.0874e-02,  1.2554e-01, -2.4839e-01,\n         -2.8324e-01,  4.7741e-01, -1.0150e+00, -1.2389e-01,  7.0153e-01,\n          5.0479e-01, -2.1854e-01,  3.3278e-01, -3.8947e-02,  5.6840e-01,\n          2.4765e-03,  4.1221e-01, -4.5733e-01,  3.9818e-01,  2.1182e-01],\n        [-5.6978e-01, -2.8303e-01,  6.2844e-01, -5.0126e-01, -4.1373e-01,\n          3.8575e-02, -7.3582e-01, -4.5884e-01, -6.5996e-01, -8.4587e-01,\n          1.5161e-01, -2.8349e-01, -2.5736e-01, -1.0861e-01,  2.7276e-01,\n          2.4449e-01, -7.6783e-01,  5.4569e-02,  1.3696e-01, -5.8347e-01,\n         -9.6441e-02, -2.8922e-01, -8.8451e-01, -6.6603e-01, -4.7134e-01,\n         -1.0699e-01, -1.1080e+00, -8.8134e-02, -4.8611e-02, -1.3631e-01,\n         -4.7960e-02,  3.3754e-01, -2.0461e-01, -5.9120e-01, -4.9328e-01,\n         -3.2873e-01, -6.5853e-01,  1.8840e-01,  7.5903e-01, -1.1353e-01,\n         -1.2391e+00, -3.1842e-01,  3.7499e-01, -7.3661e-01,  1.3012e-01,\n          2.4326e-01,  7.4628e-02, -7.5843e-01,  5.5492e-01,  2.5260e-01],\n        [ 1.9828e-01, -2.1927e-01,  1.1587e-01, -8.1529e-02, -9.7012e-01,\n         -7.5598e-01, -2.2428e-01, -3.0672e-01, -1.1779e+00, -4.5323e-01,\n         -5.8281e-01, -7.3269e-01, -1.1498e-01,  1.8533e-01,  1.7690e-01,\n          9.2935e-01, -9.5548e-02, -3.9691e-01,  2.6672e-01, -1.1356e-01,\n          7.4010e-01, -5.4229e-01, -6.3948e-03,  2.5966e-01, -2.0745e-02,\n         -3.8316e-01,  4.9610e-01,  1.0077e-01,  4.3642e-01, -1.7377e-01,\n         -3.1826e-01,  8.3811e-01, -6.0837e-01,  1.2090e-01,  7.1967e-02,\n         -4.3404e-01, -9.9839e-01, -1.4025e-01, -1.1270e+00, -1.6714e-03,\n         -4.2126e-01, -5.0354e-01, -1.4878e-01,  5.6046e-01,  8.0422e-01,\n         -1.0011e-01,  3.8311e-02,  7.5461e-01,  2.6323e-01, -9.4734e-01],\n        [-6.8842e-01,  2.0801e-01, -2.9695e-01,  1.4052e-01, -2.1762e-01,\n          1.6319e-01, -2.9260e-01,  9.1967e-01,  1.2371e-01, -5.7047e-01,\n         -2.9711e-01,  3.6139e-02, -8.5847e-02,  6.7222e-01,  4.7608e-01,\n         -6.6917e-02,  9.5268e-01, -3.3274e-01,  2.2060e-01,  1.0332e+00,\n         -4.6487e-01, -1.5301e-02, -9.6650e-01, -4.1155e-01,  2.7320e-02,\n         -1.6044e-01, -8.2882e-01,  8.4408e-04,  2.9668e-02, -5.0652e-02,\n         -4.8543e-01,  8.6943e-01,  6.7560e-02, -5.9509e-01, -1.0664e+00,\n          3.7165e-01, -2.8769e-01, -6.3426e-01,  1.0025e+00, -4.1209e-01,\n         -2.4115e-01,  1.8382e-01, -2.1366e-01,  6.2612e-01, -3.6481e-01,\n          1.0120e-01,  1.2090e-01, -1.0611e-01,  7.4887e-01,  5.7447e-01],\n        [-2.5018e-01, -2.9823e-01, -1.2244e+00,  1.0914e-02,  7.1834e-01,\n         -6.4226e-01, -5.3282e-01, -6.8323e-01,  5.2738e-01,  9.2514e-01,\n         -4.9815e-02,  1.0821e-01, -4.0392e-02,  6.2231e-01, -8.0103e-01,\n         -6.2037e-01, -1.0960e+00,  2.1601e-01, -1.0912e+00, -4.3334e-01,\n         -1.1108e-01, -3.1009e-01,  1.4231e-01, -4.2849e-01, -3.0925e-01,\n          1.3878e-01, -3.1032e-01,  4.3128e-01, -3.6598e-01, -9.2568e-01,\n          4.9374e-01, -3.5059e-02, -3.5235e-01,  3.6694e-01, -1.0718e-01,\n         -5.2268e-01, -2.1579e-01, -5.7271e-01,  2.7664e-02, -5.6156e-01,\n         -3.7572e-01,  4.1817e-03, -7.8157e-01,  2.3901e-01,  1.7176e-01,\n         -4.2624e-01, -1.2822e-01,  3.6934e-01, -6.3171e-01, -1.3858e-01]],\n       dtype=torch.float64)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for layer in net2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(layer.in_features)\n",
    "        print(layer.out_features)\n",
    "        print(layer.weight.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[-0.2094,  0.0319, -0.7622,  ..., -0.4885, -0.0992, -0.8959],\n        [-0.2159,  0.0797, -0.6359,  ..., -0.0237, -0.8760, -0.2513],\n        [-0.4455, -0.3976, -0.8292,  ..., -0.0503, -0.7132, -0.5125],\n        ...,\n        [ 0.1304,  0.1706, -0.6587,  ..., -0.5973,  0.2466, -0.6278],\n        [-0.3608, -0.3688, -0.2169,  ..., -0.0129,  0.0074,  0.0656],\n        [-0.3450, -0.0887, -0.3951,  ..., -0.2964, -0.0080, -0.0558]],\n       dtype=torch.float64, requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# torch.nn.init\n",
    "from torch.nn import init\n",
    "print(net1[0].weight)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.2167,  0.1717,  0.1476,  ..., -0.1428,  0.1155,  0.1211],\n        [ 0.2419, -0.0752, -0.0981,  ..., -0.0314, -0.0269,  0.2063],\n        [ 0.2284,  0.0365, -0.0352,  ...,  0.1686, -0.2181,  0.1994],\n        ...,\n        [-0.1991, -0.1123,  0.0811,  ...,  0.0517, -0.1563, -0.0650],\n        [ 0.1344, -0.1792, -0.0573,  ..., -0.0122, -0.2517, -0.1374],\n        [-0.1347, -0.0834,  0.0874,  ..., -0.1921, -0.0532, -0.0180]],\n       dtype=torch.float64, requires_grad=True)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "init.xavier_uniform_(net1[0].weight)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-12c6a786",
   "language": "python",
   "display_name": "PyCharm (PyTorchTutorals)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}